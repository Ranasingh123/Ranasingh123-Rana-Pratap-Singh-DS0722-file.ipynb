{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e77b2f",
   "metadata": {},
   "source": [
    "Web Scraping Assignment - 3\n",
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caa8ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "s = Service(r\"C:\\Users\\Dell\\Downloads\\chromedriver_win32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32782d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_11436\\2679670127.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "188d3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the webpage of mentioned url\n",
    "url = \"https://www.amazon.in/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the product that we want to search\n",
    "user_input = input('Enter the product that we want to search : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching the web element for user input\n",
    "search = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "search\n",
    "\n",
    "# sending the user input to search bar\n",
    "search.send_keys(user_input)\n",
    "\n",
    "# locating the search button using xpath\n",
    "search_btn = driver.find_element_by_xpath(\"//div[@class='nav-search-submit nav-sprite']/span/input\")\n",
    "\n",
    "# clicking on search button\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869eb858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad2fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6473db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2 : In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product vertical has less than 3 pages in search results then scrape all the products available under that product vertical. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Rating\", \"No. of Ratings\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Other Details\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching URLs to open the pages\n",
    "urls = []          # empty list\n",
    "for i in range(0,3):      # for loop to scrape 3 pages\n",
    "    page_url = driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "    for i in page_url:\n",
    "        urls.append(i.get_attribute(\"href\"))\n",
    "        next_btn = driver.find_element_by_xpath(\"//li[@class='a-last']/a\")\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d7a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cb5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making empty list and fetching required data\n",
    "brand_name = []\n",
    "product_name = []\n",
    "ratings = []\n",
    "num_ratings = []\n",
    "prices = []\n",
    "exchange = []\n",
    "exp_delivery = []\n",
    "availability = []\n",
    "other_details = []\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    #fetching brand name \n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath(\"//a[@id='bylineInfo']\")\n",
    "        brand_name.append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        brand_name.append('-')\n",
    "    \n",
    "    \n",
    "    # fetching Name of the Product\n",
    "    try:\n",
    "        product = driver.find_element_by_xpath(\"//span[@id='productTitle']\")\n",
    "        product_name.append(product.text)\n",
    "    except NoSuchElementException:\n",
    "        product_name.append('-')\n",
    "        \n",
    "        \n",
    "\n",
    "     #fetching ratings\n",
    "    try:\n",
    "        rating = driver.find_element_by_xpath(\"//span[@class='a-size-base a-nowrap']/span\")\n",
    "        ratings.append(rating.text)\n",
    "    except NoSuchElementException:\n",
    "        ratings.append('-')\n",
    "        \n",
    " \n",
    "    #fetching  no of ratings\n",
    "    try:\n",
    "        num_rating = driver.find_element_by_xpath(\"//span[@id='acrCustomerReviewText']\")\n",
    "        num_ratings.append(num_rating.text)\n",
    "    except NoSuchElementException:\n",
    "        num_ratings.append('-')\n",
    "        \n",
    "\n",
    "    #fetching price of the product\n",
    "    try:\n",
    "        price = driver.find_element_by_xpath(\"//td[@class='a-span12']\")\n",
    "        prices.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        prices.append('-')\n",
    "        \n",
    "        \n",
    "    #fetching return/exchange\n",
    "    try:\n",
    "        exch = driver.find_element_by_xpath(\"//span[@class='a-declarative']/div/a\")\n",
    "        exchange.append(exch.text)\n",
    "    except NoSuchElementException:\n",
    "        exchange.append('-')\n",
    "        \n",
    "\n",
    "    #fetching expected delivery\n",
    "    try:\n",
    "        delivery = driver.find_element_by_xpath(\"//div[@class='a-section a-spacing-mini']/b\")\n",
    "        exp_delivery.append(delivery.text)\n",
    "    except NoSuchElementException:\n",
    "        exp_delivery.append('-')\n",
    "        \n",
    "\n",
    "    #fetching availability information\n",
    "    try:\n",
    "        avail = driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-success']\")\n",
    "        availability.append(avail.text)\n",
    "    except NoSuchElementException:\n",
    "        availability.append('-')\n",
    "        \n",
    "    #other details\n",
    "    try:\n",
    "        oth_det = driver.find_element_by_xpath(\"//ul[@class='a-unordered-list a-vertical a-spacing-mini']\")\n",
    "        other_details.append(oth_det.text)\n",
    "    except NoSuchElementException:\n",
    "        other_details.append('-')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(brand_name),\n",
    "len(product_name),\n",
    "len(ratings),\n",
    "len(num_ratings),\n",
    "len(prices),\n",
    "len(exchange),\n",
    "len(exp_delivery),\n",
    "len(availability),\n",
    "len(other_details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DataFrame for the scraped data\n",
    "\n",
    "guitar = pd.DataFrame({})\n",
    "guitar['Brand Name'] = brand_name\n",
    "guitar['Name of the Product'] = product_name\n",
    "guitar['Rating'] = ratings\n",
    "guitar['No. of Ratings'] = num_ratings\n",
    "guitar['Price'] = prices\n",
    "guitar['Return/Exchange'] = exchange\n",
    "guitar['Expected Delivery'] = exp_delivery\n",
    "guitar['Availability'] = availability\n",
    "guitar['Other Details'] = other_details\n",
    "guitar['Product URL'] = urls\n",
    "guitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the data in csv\n",
    "guitar.to_csv(\"Guitar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a422c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692dd11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cfd0754",
   "metadata": {},
   "source": [
    "Q3 : Write a python program to access the search bar and search button on images.google.com and scrape 100 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf24d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def3b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geting the webpage of mentioned url\n",
    "url = \"http://images.google.com/\"\n",
    "\n",
    "# creating empty list\n",
    "urls = []\n",
    "data = []\n",
    "\n",
    "search_item = [\"Fruits\",\"Cars\",\"Machine Learning\"]\n",
    "for item in search_item:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # finding webelement for search_bar\n",
    "    search_bar = driver.find_element_by_tag_name(\"input\")\n",
    "    \n",
    "    # sending keys to get the keyword for search bar\n",
    "    search_bar.send_keys(str(item))\n",
    "    \n",
    "    # clicking on search button\n",
    "    search_button = driver.find_element_by_xpath(\"//button[@class='Tg7LZd']\").click()\n",
    "    \n",
    "    # scroling down the webpage to get some more images\n",
    "    for _ in range(500):\n",
    "        driver.execute_script(\"window.scrollBy(0,100)\")\n",
    "        \n",
    "        imgs = driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\")\n",
    "    img_url = []\n",
    "    for image in imgs:\n",
    "        source = image.get_attribute('src')\n",
    "        if source is not None:\n",
    "            if(source[0:4] == 'http'):\n",
    "                img_url.append(source)\n",
    "    for i in img_url[:100]:\n",
    "        urls.append(i)\n",
    "        \n",
    "for i in range(len(urls)):\n",
    "    if i >= 300:\n",
    "        break\n",
    "    print(\"Doenloading {0} of {1} images\" .format(i,300))\n",
    "    response = requests.get(urls[i])\n",
    "    \n",
    "    file = open(r\"E:\\google\\images\"+str(i)+\".jpg\",\"wb\")\n",
    "    \n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea52ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93d9f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70300373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89b13dec",
   "metadata": {},
   "source": [
    "Q4 : Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Display Resolution”, “Processor”, “Processor Cores”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d47276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec605151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the webpage of mentioned url\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00251f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing login popup button\n",
    "lonin_x_btn = driver.find_element_by_xpath(\"//div[@class='_2QfC02']//button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627595dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for web element\n",
    "search_bar = driver.find_element_by_xpath(\"//input[@class='_3704LK']\")\n",
    "\n",
    "# sending keys to search product\n",
    "search_bar.send_keys(\"pixel 4A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location the search button using xpath\n",
    "search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "\n",
    "# clicking on search button\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4335fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching 1st page of URLs of smartphone\n",
    "page1_url = []\n",
    "urls = driver.find_elements_by_xpath(\"//a[@class='_1fQZEK']\")\n",
    "for url in urls:\n",
    "    page1_url.append(url.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(page1_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "Smartphones = ({})\n",
    "Smartphones['Brand'] = []\n",
    "Smartphones['Phone name'] = []\n",
    "Smartphones['Colour'] = []\n",
    "Smartphones['RAM'] = []\n",
    "Smartphones['Storage(ROM)'] = []\n",
    "Smartphones['Primary Camera'] = []\n",
    "Smartphones['Secondary Camera'] = []\n",
    "Smartphones['Display Size'] = []\n",
    "Smartphones['Display Resolution'] = []\n",
    "Smartphones['Processor'] = []\n",
    "Smartphones['Processor Cores'] = []\n",
    "Smartphones['Battery Capacity'] = []\n",
    "Smartphones['Price'] = []\n",
    "Smartphones['URL'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b36609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping data from each url of page 1\n",
    "for url in page1_url:\n",
    "    driver.get(url)\n",
    "    print(\"Scraping URL = \",url)\n",
    "    Smartphones['URL'].append(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    #clicking on read more button to get more information\n",
    "    try:\n",
    "        read_more = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _1FH0tX']\")\n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception occured while moving to next page\")\n",
    "    \n",
    "    #scraping brand name of smartphone\n",
    "    try:\n",
    "        brand_tags = driver.find_element_by_xpath(\"//span[@class='B_NuCI']\")\n",
    "        Smartphones['Brand'].append(brand_tags.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Brand'].append('-')\n",
    "    \n",
    "    \n",
    "    # scraping name of smartphones\n",
    "    try:\n",
    "        name_tags = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][1]/table/tbody/tr[3]/td[2]/ul/li\")\n",
    "        Smartphones['Phone name'].append(name_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Phone name'].append('-')\n",
    "        \n",
    "    #scraping colour of smartphone\n",
    "    try:\n",
    "        color_tags = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][1]/table/tbody/tr[4]/td[2]/ul/li\")\n",
    "        Smartphones['Colour'].append(color_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Colour'].append('-')\n",
    "        \n",
    "    # scraping RAM data of smartphone\n",
    "    try:\n",
    "        ram_tags = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][4]/table[1]/tbody/tr[2]/td[2]/ul/li\")\n",
    "        Smartphones['RAM'].append(ram_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['RAM'].append('-')\n",
    "        \n",
    "    #scraping ROM data of smartphones\n",
    "    try:\n",
    "        rom = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][4]/table[1]/tbody/tr[1]/td[2]/ul/li\")\n",
    "        Smartphones['Storage(ROM)'].append(rom.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Storage(ROM)'].append('-')\n",
    "        \n",
    "    # scraping  Primary camera data of smartphone\n",
    "    try:\n",
    "        pri =driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][5]/table[1]/tbody/tr[2]/td[2]/ul/li\")\n",
    "        Smartphones['Primary Camera'].append(pri.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Primary Camera'].append('-')\n",
    "        \n",
    "    # scraping secondary camera data of smartphone\n",
    "    try:\n",
    "        sec = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][5]/table[1]/tbody/tr[6]/td[1]\")\n",
    "        if sec != 'Secondary Camera' :\n",
    "            if driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][5]/table[1]/tbody/tr[5]/td[1]\").text == \"Secondary Camera\":\n",
    "                sec_cam =driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][5]/table[1]/tbody/tr[5]/td[2]/ul/li\")\n",
    "            else :\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            sec_cam = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][5]/table[1]/tbody/tr[6]/td[2]/ul/li\")\n",
    "        Smartphones['Secondary Camera'].append(sec_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Secondary Camera'].append('-')\n",
    "        \n",
    "    \n",
    "    #scraping display size data of smartphone\n",
    "    try:\n",
    "        disp = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][2]/div\")\n",
    "        if disp.text != 'Display Features' : raise NoSuchElementException\n",
    "        disp_size = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][2]/table[1]/tbody/tr[1]/td[2]/ul/li\")\n",
    "        Smartphones['Display Size'].append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Display Size'].append('-')\n",
    "        \n",
    "    \n",
    "    #scraping display resolution of smartphone\n",
    "    try:\n",
    "        disp = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][2]/div\")\n",
    "        if disp.text != 'Display Features' : raise NoSuchElementException\n",
    "        disp_reso = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][2]/table[1]/tbody/tr[2]/td[2]/ul/li\")\n",
    "        Smartphones['Display Resolution'].append(disp_reso.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Display Resolution'].append('-')\n",
    "        \n",
    "        \n",
    "    #scraping processor of smartphone\n",
    "    try:\n",
    "        pro = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][3]/table[1]/tbody/tr[2]/td[1]]\")\n",
    "        if pro.text != 'Processor Type' : raise NoSuchElementException\n",
    "        processor = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][3]/table[1]/tbody/tr[2]/td[2]/ul/li\")\n",
    "        Smartphones['Processor'].append(processor.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Processor'].append('-')\n",
    "    \n",
    "        \n",
    "       \n",
    "    # scraping processor core of smartphone\n",
    "    try:\n",
    "        core = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][3]/table[1]/tbody/tr[3]/td[1]\")\n",
    "        if core.text != 'Processor Core' :\n",
    "            core = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][3]/table[1]/tbody/tr[2]/td[1]\")\n",
    "            if core.text != 'Processor Core' :\n",
    "                raise NoSuchElementException\n",
    "            else :\n",
    "                cores = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][3]/table[1]/tbody/tr[2]/td[2]/ul/li\")\n",
    "        else :\n",
    "            cores = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][3]/table[1]/tbody/tr[3]/td[2]/ul/li\")\n",
    "        Smartphones['Processor Cores'].append(disp_reso.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Processor Cores'].append('-')\n",
    "        \n",
    "        \n",
    "        \n",
    "    # scraping the battery capacity of smartphone\n",
    "    try:\n",
    "        if driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][10]/div\").text != \"Battery & Power Features\" :\n",
    "            if driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][9]/div\").text == \"Battery & Power Features\" :\n",
    "                bat_tags = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][9]/table/tbody/tr/td[1]\")\n",
    "                if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_capa = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][9]/table/tbody/tr/td[2]/ul/li\")\n",
    "            elif driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][8]/div\").text == \"Battery & Power Features\" :\n",
    "                bat_tags = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][8]/table/tbody/tr/td[1]\")\n",
    "                if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_capa = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][8]/table/tbody/tr/td[2]/ul/li\")\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_tags = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][10]/table/tbody/tr/td[1]\")\n",
    "            if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "            bat_capa = driver.find_element_by_xpath(\"//div[@class='_3k-BhJ'][10]/table/tbody/tr/td[2]/ul/li\")\n",
    "        Smartphones['Battery Capacity'].append(bat_capa.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Battery Capacity'].append('-')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping price of smartphone\n",
    "    try:\n",
    "        price_tags = driver.find_element_by_xpath(\"//div[@class='_30jeq3 _16Jk6d']\")\n",
    "        Smartphones['Price'].append(price_tags.text)\n",
    "    except NoSuchElementException:\n",
    "          Smartphones['Price'].append('-')         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb61dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking lengths of all scraped data\n",
    "\n",
    "print(len(Smartphones['Brand']),len(Smartphones['Phone name']), len(Smartphones['Colour']),len(Smartphones['RAM']),len(Smartphones['Storage(ROM)']),len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# framing the DataFrame\n",
    "\n",
    "df = pd.DataFrame.from_dict(Smartphones)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data in csv\n",
    "df.to_csv(\"smartphones.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46521b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37317bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1805a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314dd7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5 : Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f00b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting mentioned url and opening google maps web page\n",
    "url = 'https://www.google.co.in/maps'\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c80520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the city name in search bar\n",
    "City = input('Enter City name that has to be searched : ')\n",
    "search_bar = driver.find_element_by_id('searchboxinput')\n",
    "search_bar.click()\n",
    "time.sleep(2)\n",
    "\n",
    "#sending keys to find cities\n",
    "search_bar.send_keys(City)\n",
    "\n",
    "#checking for webelement and clicking on search button\n",
    "search_btn = driver.find_element_by_id(\"searchbox-searchbutton\")\n",
    "search_btn.click()\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    url_str = driver.current_url\n",
    "    print(\"URL Extracted: \", url_str)\n",
    "    latitude_longitude = re.findall(r'@(.*)data',url_str)\n",
    "    if len(latitude_longitude):\n",
    "        lat_lng_list = latitude_longitude[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            latitude = lat_lng_list[0]\n",
    "            longitude = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(latitude, longitude))\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded023d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e17a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a10b28e9",
   "metadata": {},
   "source": [
    "Q6 : Write a program to scrap details of all the funding deals for second quarter (i.e. July 20 – September 20) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ada88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88be68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the url track.in\n",
    "url = \"https://trak.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493575fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting xpath for funding deals and clicking on the button\n",
    "fund_button = driver.find_element_by_xpath(\"//li[@id='menu-item-51510']/a\").get_attribute('href')\n",
    "driver.get(fund_button)\n",
    "\n",
    "#Empty Lists\n",
    "fund_deals = {}\n",
    "fund_deals['Date'] = []\n",
    "fund_deals['Startup Name'] = []\n",
    "fund_deals['Industry/Vertical'] = []\n",
    "fund_deals['Sub_Vertical'] = []\n",
    "fund_deals['Location'] = []\n",
    "fund_deals['Investor'] = []\n",
    "fund_deals['Investment Type'] = []\n",
    "fund_deals['Amount(in USD)'] = []\n",
    "\n",
    "\n",
    "for i in range(48,51):\n",
    "    \n",
    "    # scraping data of data\n",
    "    date = driver.find_elements_by_xpath(\"//table[@id='tablepress-{}']/tbody/tr/td[2]\".format(i))\n",
    "    for d in date:\n",
    "        fund_deals['Date'].append(d.text)\n",
    "        \n",
    "    # scraping data of startup name\n",
    "    startup_name = driver.find_elements_by_xpath(\"//table[@id='tablepress-{}']/tbody/tr/td[3]\".format(i))\n",
    "    for name in startup_name:\n",
    "        fund_deals['Startup Name'].append(name.text)\n",
    "        \n",
    "    \n",
    "    #scraping data of industry or vertical\n",
    "    industry = driver.find_elements_by_xpath(\"//table[@id='tablepress-{}']/tbody/tr/td[4]\".format(i))\n",
    "    for ind in industry:\n",
    "        fund_deals['Industry/Vertical'].append(ind.text)\n",
    "        \n",
    "    \n",
    "    #scraping data of sub-vertical\n",
    "    sub_vertical = driver.find_elements_by_xpath(\"//table[@id='tablepress-{}']/tbody/tr/td[5]\".format(i))\n",
    "    for sv in sub_vertical:\n",
    "        fund_deals['Sub_Vertical'].append(sv.text)\n",
    "        \n",
    "        \n",
    "    # scraping data of location\n",
    "    location = driver.find_elements_by_xpath(\"//table[@id='tablepress-{}']/tbody/tr/td[6]\".format(i))\n",
    "    for loc in location:\n",
    "        fund_deals['Location'].append(loc.text)\n",
    "        \n",
    "        \n",
    "    # scraping data of investor\n",
    "    investor = driver.find_elements_by_xpath(\"//table[@id='tablepress-{}']/tbody/tr/td[7]\".format(i))\n",
    "    for invest in investor:\n",
    "        fund_deals['Investor'].append(invest.text)\n",
    "        \n",
    "        \n",
    "    # scraping data of investment type\n",
    "    investment_type = driver.find_elements_by_xpath(\"//table[@id='tablepress-{}']/tbody/tr/td[8]\".format(i))\n",
    "    for invtype in investment_type:\n",
    "        fund_deals['Investment Type'].append(invtype.text)\n",
    "        \n",
    "        \n",
    "    # scraping data of amount\n",
    "    amount = driver.find_elements_by_xpath(\"//table[@id='tablepress-{}']/tbody/tr/td[9]\".format(i))\n",
    "    for amt in amount:\n",
    "        fund_deals['Amount(in USD)'].append(amt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af445ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking lengths of all scraped data\n",
    "print(len(fund_deals['Date']),\n",
    "len(fund_deals['Startup Name']),\n",
    "len(fund_deals['Industry/Vertical']),\n",
    "len(fund_deals['Sub_Vertical']),\n",
    "len(fund_deals['Location']),\n",
    "len(fund_deals['Investor']),\n",
    "len(fund_deals['Investment Type']),\n",
    "len(fund_deals['Amount(in USD)'] \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7921d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating DataFrame for scraped data\n",
    "fund_data = pd.DataFrame(fund_deals)\n",
    "fund_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56560b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data in csv file\n",
    "fund_data.to_csv(\"trak_in.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cfcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5abf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31425c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c8a6e2d",
   "metadata": {},
   "source": [
    "Q7 : Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ddc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2136e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the url digit.in\n",
    "url = \"https://www.digit.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24831e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching for best Laptop\n",
    "best_gam_laptops = driver.find_element_by_xpath(\"//div[@class='listing_container']//ul//li[9]\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd348fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "Laptop_Name = []\n",
    "Operating_sys = []\n",
    "Display = []\n",
    "Processor = []\n",
    "Memory = []\n",
    "Weight = []\n",
    "Dimensions = []\n",
    "Graph_proc = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811306a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the data of laptop names\n",
    "laptop_name = driver.find_elements_by_xpath(\"//div[@class='right-container']/div/a/h3\")\n",
    "for name in laptop_name:\n",
    "    Laptop_Name.append(name.text)\n",
    "    \n",
    "#scraping the data of operating system\n",
    "try:\n",
    "    op_sys = driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "    for os in op_sys:\n",
    "        Operating_sys.append(os.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#scraping data of display of the Laptop\n",
    "try:\n",
    "    display = driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "    for disp in display:\n",
    "        Display.append(disp.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of processor\n",
    "try:\n",
    "    processor = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[5]/td[3]\")\n",
    "    for pro in processor:\n",
    "        Processor.append(pro.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping the data of memory\n",
    "try:\n",
    "    memory = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")\n",
    "    for memo in memory:\n",
    "        Memory.append(memo.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of weight\n",
    "try:\n",
    "    weight = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3]\")\n",
    "    for wgt in weight:\n",
    "        Weight.append(wgt.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of dimensions\n",
    "try:\n",
    "    dimension = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[8]/td[3]\")\n",
    "    for dim in dimension:\n",
    "        Dimensions.append(dim.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of graph processor\n",
    "try:\n",
    "    graph = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[9]/td[3]\")\n",
    "    for gra in graph:\n",
    "        Graph_proc.append(gra.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping the data of price\n",
    "try:\n",
    "    price = driver.find_elements_by_xpath(\"//td[@class='smprice']\")\n",
    "    for pri in price:\n",
    "        Price.append(pri.text.replace('₹ ','Rs'))\n",
    "except NoSuchElementException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7083c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Laptop_Name),\n",
    "len(Operating_sys),\n",
    "len(Display),\n",
    "len(Processor),\n",
    "len(Memory),\n",
    "len(Weight),\n",
    "len(Dimensions),\n",
    "len(Graph_proc),\n",
    "len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating DataFrame for scraped data\n",
    "Gaming_Laptop=pd.DataFrame({})\n",
    "Gaming_Laptop['Laptop Name'] = Laptop_Name\n",
    "Gaming_Laptop['Operating System'] =Operating_sys\n",
    "Gaming_Laptop['Display'] = Display\n",
    "Gaming_Laptop['Processor'] = Processor\n",
    "Gaming_Laptop['Memory'] = Memory\n",
    "Gaming_Laptop['Weight'] = Weight\n",
    "Gaming_Laptop['Dimensions'] = Dimensions\n",
    "Gaming_Laptop['Graphical Processor'] = Graph_proc\n",
    "Gaming_Laptop['Price'] = Price\n",
    "Gaming_Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data to csv\n",
    "Gaming_Laptop.to_csv(\"Gaming_Laptops.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae490d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f02c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c7b0f4c",
   "metadata": {},
   "source": [
    "Q8 : Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dafc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e02feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the specified url\n",
    "url = \"https://www.forbes.com/?sh=41bd46d2254c\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16301e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get option button from the page\n",
    "opt_btn = driver.find_element_by_xpath(\"//div[@class='header__left']//button\")\n",
    "opt_btn.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#select billionaires from options\n",
    "blns = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "blns.click()\n",
    "time.sleep(3)\n",
    "#select world billionaire\n",
    "bln_list = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "bln_list.click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a11ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping required data from the web page\n",
    "# creating empty lists\n",
    "Rank = []\n",
    "Person_Name = []\n",
    "Net_worth = []\n",
    "Age = []\n",
    "Citizenship = []\n",
    "Source = []\n",
    "Industry = []\n",
    "\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # scraping the data of rank of the billionaires\n",
    "    rank_tag = driver.find_elements_by_xpath(\"//div[@class='rank']\")\n",
    "    for rank in rank_tag:\n",
    "        Rank.append(rank.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    " \n",
    "    # scraping the data  of names of the billionaires\n",
    "    name_tag = driver.find_elements_by_xpath(\"//div[@class='personName']/div\")\n",
    "    for name in name_tag:\n",
    "        Person_Name.append(name.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of age of the billionaires\n",
    "    age_tag = driver.find_elements_by_xpath(\"//div[@class='age']/div\")\n",
    "    for age in age_tag:\n",
    "        Age.append(age.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of citizenship of the billionaires\n",
    "    cit_tag = driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship']\")\n",
    "    for cit in cit_tag:\n",
    "        Citizenship.append(cit.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of source of income of the billionaires\n",
    "    sour_tag = driver.find_elements_by_xpath(\"//div[@class='source']\")\n",
    "    for sour in sour_tag:\n",
    "        Source.append(sour.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping data of industry of the billionaires\n",
    "    ind_tag = driver.find_elements_by_xpath(\"//div[@class='category']//div\")\n",
    "    for ind in ind_tag:\n",
    "        Industry.append(ind.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping data of net_worth of billionaires\n",
    "    net_tag = driver.find_elements_by_xpath(\"//div[@class='netWorth']/div\")\n",
    "    for net in net_tag:\n",
    "        Net_worth.append(net.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # clicking on next button\n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath(\"//button[@class='pagination-btn pagination-btn--next ']\")\n",
    "        next_button.click(\n",
    "    except:\n",
    "        break\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4370e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Rank),\n",
    "len(Person_Name),\n",
    "len(Net_worth),\n",
    "len(Age),\n",
    "len(Citizenship),\n",
    "len(Source),\n",
    "len(Industry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# framing Data\n",
    "Billionaires = pd.DataFrame({})\n",
    "Billionaires['Rank'] = Rank\n",
    "Billionaires['Name'] = Person_Name\n",
    "Billionaires['Net Worth'] = Net_worth\n",
    "Billionaires['Age'] = Age\n",
    "Billionaires['Citizenship'] = Citizenship\n",
    "Billionaires['Source'] = Source\n",
    "Billionaires['Industry'] = Industry\n",
    "Billionaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataset in csv\n",
    "Billionaires.to_csv('Forbes_Billionaires.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c3f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ed424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb7467aa",
   "metadata": {},
   "source": [
    "Q9 : Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the youtube.com\n",
    "url = \"https://www.youtube.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for search bar\n",
    "search_bar = driver.find_element_by_xpath(\"//div[@class='ytd-searchbox-spt']/input\")\n",
    "search_bar.send_keys(\"GOT\")      # entering video name\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search button\n",
    "search_btn = driver.find_element_by_id(\"search-icon-legacy\")\n",
    "search_btn.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8311aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on first video\n",
    "video = driver.find_element_by_xpath(\"//yt-formatted-string[@class='style-scope ytd-video-renderer']\")\n",
    "video.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 times we scroll down by 10000 in order to generate more comments\n",
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30548d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "comments = []\n",
    "comment_time = []\n",
    "Time = []\n",
    "Likes = []\n",
    "No_of_Likes = []\n",
    "\n",
    "# scrape comments\n",
    "cm = driver.find_elements_by_id(\"content-text\")\n",
    "for i in cm:\n",
    "    if i.text is None:\n",
    "        comments.append(\"--\")\n",
    "    else:\n",
    "        comments.append(i.text)\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "# scrape time when comment was posted\n",
    "tm = driver.find_elements_by_xpath(\"//a[contains(text(),'ago')]\")\n",
    "for i in tm:\n",
    "    Time.append(i.text)\n",
    "    \n",
    "for i in range(0,len(Time),2):\n",
    "    comment_time.append(Time[i])\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "# scrape the comment likes\n",
    "like = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for i in like:\n",
    "    Likes.append(i.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(comments),len(comment_time),len(No_of_Likes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe for scraped data\n",
    "\n",
    "Youtube = pd.DataFrame({})\n",
    "Youtube['Comment'] = comments[:500]\n",
    "Youtube['Comment Time'] = comment_time[:500]\n",
    "Youtube['Comment Upvotes'] = No_of_Likes[:500]\n",
    "Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the dataframe to csv\n",
    "Youtube.to_csv(\"Youtube GOT Comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bcc830",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acab96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763acb41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c164fba9",
   "metadata": {},
   "source": [
    "Q10 : Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description.\n",
    "# connecting to the webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:/Users/HP/Downloads/chromedriver_win32 (1)/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac462abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the web page of mentioned url\n",
    "url = \"https://www.hostelworld.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locating the location search bar\n",
    "search_bar = driver.find_element_by_id(\"search-input-field\")\n",
    "\n",
    "# entering London in search bar\n",
    "search_bar.send_keys(\"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2314da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select London\n",
    "London = driver.find_element_by_xpath(\"//ul[@id='predicted-search-results']//li[2]\")\n",
    "#clicking on button\n",
    "London.click()\n",
    "\n",
    "# do click on Let's Go button\n",
    "search_btn = driver.find_element_by_id('search-button')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335515d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list & find required data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description = []\n",
    "url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a22815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the required informations\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping  hostel name\n",
    "    try:\n",
    "        name = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "        \n",
    "        \n",
    "    # scraping distance from city centre\n",
    "    try:\n",
    "        dist = driver.find_elements_by_xpath(\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in name:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "        \n",
    "   \n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):   \n",
    "    # scraping privates from price\n",
    "        try:\n",
    "            pvt_price = driver.find_element_by_xpath(\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "   \n",
    "\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):          \n",
    "    # scraping dorms from price\n",
    "        try:\n",
    "            dorms = driver.find_element_by_xpath(\"//a[@class='prices']//div[2]/div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "            \n",
    "            \n",
    "    # scraping facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements_by_xpath(\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements_by_xpath(\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text)\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "     \n",
    "            \n",
    "    #fetching url of each hostel\n",
    "    p_url = driver.find_elements_by_xpath(\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        url.append(i.get_attribute(\"href\"))\n",
    "        \n",
    "for i in url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "\n",
    "    # scraping ratings\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "        \n",
    "        \n",
    "    # scraping total review\n",
    "    try:\n",
    "        rws = driver.find_element_by_xpath(\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "        \n",
    "        \n",
    "    # fetching over all review\n",
    "    try:\n",
    "        overall = driver.find_element_by_xpath(\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "        \n",
    "        \n",
    "    # fetching property description\n",
    "    try:\n",
    "        disc = driver.find_element_by_xpath(\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "    \n",
    "    # do click on show more button for description\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//a[@class='toggle-content']\").click()\n",
    "        time.sleep(4)\n",
    "    except NoSuchElementException:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af363e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hostel_name),\n",
    "len(distance),\n",
    "len(pvt_prices),\n",
    "len(dorms_price),\n",
    "len(rating),\n",
    "len(reviews),\n",
    "len(over_all),\n",
    "len(facilities),\n",
    "len(description),\n",
    "len(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4be7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating DataFrame\n",
    "Hostel = pd.DataFrame({})\n",
    "Hostel['Hostel Name'] = hostel_name\n",
    "Hostel['Distance from City Centre'] = distance\n",
    "Hostel['Ratings'] = rating\n",
    "Hostel['Total Reviews'] = reviews\n",
    "Hostel['Overall Reviews'] = over_all\n",
    "Hostel['Privates from Price'] = pvt_prices\n",
    "Hostel['Dorms from Price'] = dorms_price\n",
    "Hostel['Facilities'] = facilities[:74]\n",
    "Hostel['Description'] = description\n",
    "Hostel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e0337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataset to csv\n",
    "Hostel.to_csv(\"London_Hostels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ff15f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
